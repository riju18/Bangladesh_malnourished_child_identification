{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('finalChildMalnutrition.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df[['fever','Mother_BMI','diar','Father_Edu','currently_working_mot']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division</th>\n",
       "      <th>Age at 1st Birth</th>\n",
       "      <th>Child Age</th>\n",
       "      <th>stunting</th>\n",
       "      <th>underweight</th>\n",
       "      <th>wasting</th>\n",
       "      <th>diar</th>\n",
       "      <th>fever</th>\n",
       "      <th>ari</th>\n",
       "      <th>Mother_BMI</th>\n",
       "      <th>Birth_Order</th>\n",
       "      <th>Mother_edu</th>\n",
       "      <th>wealth_index_cat</th>\n",
       "      <th>Father_Edu</th>\n",
       "      <th>residence</th>\n",
       "      <th>sex</th>\n",
       "      <th>currently_working_mot</th>\n",
       "      <th>Breastfeeding</th>\n",
       "      <th>household_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6955</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6959</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6960</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6961</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6962</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.369215</td>\n",
       "      <td>0</td>\n",
       "      <td>21.694767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250862</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Division  Age at 1st Birth  Child Age  stunting  underweight  wasting  \\\n",
       "6955         7                22         40         0            0        0   \n",
       "6956         7                22         55         0            0        0   \n",
       "6957         7                19         31         1            0        0   \n",
       "6958         7                19         59         1            1        0   \n",
       "6959         7                17         42         0            0        0   \n",
       "6960         7                17         58         0            0        1   \n",
       "6961         7                18         22         1            0        0   \n",
       "6962         7                22         49         0            0        0   \n",
       "6963         7                18         53         1            1        0   \n",
       "6964         7                18         32         1            1        0   \n",
       "\n",
       "          diar     fever  ari  Mother_BMI  Birth_Order  Mother_edu  \\\n",
       "6955  0.000000  0.000000    0   21.694767            2           1   \n",
       "6956  0.000000  0.000000    0   21.694767            1           1   \n",
       "6957  0.000000  1.000000    0   21.694767            3           1   \n",
       "6958  0.000000  0.369215    0   21.694767            2           1   \n",
       "6959  0.048426  0.369215    0   21.694767            3           0   \n",
       "6960  0.048426  0.369215    0   21.694767            2           0   \n",
       "6961  0.048426  0.369215    0   21.694767            1           1   \n",
       "6962  0.048426  0.369215    0   21.694767            1           1   \n",
       "6963  0.048426  0.369215    0   21.694767            3           0   \n",
       "6964  0.048426  0.369215    0   21.694767            1           0   \n",
       "\n",
       "      wealth_index_cat  Father_Edu  residence  sex  currently_working_mot  \\\n",
       "6955                 1    0.000000          0    0               0.000000   \n",
       "6956                 1    0.000000          0    0               0.000000   \n",
       "6957                 1    0.000000          0    1               0.000000   \n",
       "6958                 1    0.000000          0    1               0.000000   \n",
       "6959                 1    1.000000          0    1               0.000000   \n",
       "6960                 1    0.000000          0    0               0.000000   \n",
       "6961                 1    0.000000          0    0               0.000000   \n",
       "6962                 1    0.000000          0    0               0.000000   \n",
       "6963                 1    0.449088          0    0               0.000000   \n",
       "6964                 1    0.449088          0    0               0.250862   \n",
       "\n",
       "      Breastfeeding  household_no  \n",
       "6955              0             5  \n",
       "6956              0             5  \n",
       "6957              0             6  \n",
       "6958              0             6  \n",
       "6959              0            17  \n",
       "6960              0            17  \n",
       "6961              1            17  \n",
       "6962              0             3  \n",
       "6963              0             6  \n",
       "6964              0             6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5978\n",
      "1     987\n",
      "Name: wasting, dtype: int64\n",
      "0    4450\n",
      "1    2515\n",
      "Name: stunting, dtype: int64\n",
      "0    4727\n",
      "1    2238\n",
      "Name: underweight, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# wasting\n",
    "# -----------\n",
    "\n",
    "print(df['wasting'].value_counts())\n",
    "\n",
    "# underweight\n",
    "# -----------\n",
    "\n",
    "print(df['stunting'].value_counts())\n",
    "\n",
    "# stuting\n",
    "# -----------\n",
    "print(df['underweight'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df.iloc[:, [1,2,6,7,8,9,10,11,12,13,14,15,16,17,18]].values\n",
    "\n",
    "y_stun = df.iloc[:, 3].values\n",
    "y_underweight = df.iloc[:, 4].values\n",
    "y_wast = df.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus = SMOTETomek(ratio = 'auto')\n",
    "smote = SMOTE('minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y_wast = rus.fit_sample(X, y_wast)\n",
    "X_sm, y_wast_sm = smote.fit_sample(X, y_wast)\n",
    "X_sm1, y_underweight_sm = smote.fit_sample(X, y_underweight)\n",
    "X_sm2, y_stun_sm = smote.fit_sample(X, y_stun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11956, 15)\n",
      "(9454, 15)\n",
      "(8900, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_sm.shape)\n",
    "print(X_sm1.shape)\n",
    "print(X_sm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_weight = class_weight.compute_class_weight('balanced', np.unique(y_wast), y_wast)\n",
    "c_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_underweight), y_underweight)\n",
    "c_weight2 = class_weight.compute_class_weight('balanced', np.unique(y_stun), y_stun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_wast, x_test_wast, y_train_wast, y_test_wast = train_test_split(X_sm, y_wast_sm, test_size = 0.1, random_state = 41)\n",
    "x_train_under, x_test_under, y_train_under, y_test_under = train_test_split(X_sm1, y_underweight_sm, test_size = 0.1, random_state = 41)\n",
    "x_train_stun, x_test_stun, y_train_stun, y_test_stun = train_test_split(X_sm2, y_stun_sm, test_size = 0.1, random_state = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_stun = sc.fit_transform(x_train_stun)\n",
    "x_train_underWeight = sc.fit_transform(x_train_under)\n",
    "x_train_wast = sc.fit_transform(x_train_wast)\n",
    "\n",
    "x_test_stun = sc.transform(x_test_stun)\n",
    "x_test_underWeight = sc.transform(x_test_under)\n",
    "x_test_wast = sc.transform(x_test_wast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential              # initialize the neural network\n",
    "from keras.layers import Dense, Dropout  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\This Pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 1st hidden layer & input layer\n",
    "\n",
    "classifiers.add(Dense(8, input_shape=(15,), activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd hidden layer\n",
    "\n",
    "classifiers.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3rd hidden layer\n",
    "\n",
    "# classifiers.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4th hidden layer\n",
    "\n",
    "# classifiers.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# # classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5th hidden layer\n",
    "\n",
    "# classifiers.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6th hidden layer\n",
    "\n",
    "# classifiers.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding output layer\n",
    "#--------------------------\n",
    "\n",
    "classifiers.add(Dense(1, activation='sigmoid',kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling ANN\n",
    "#----------------\n",
    "\n",
    "classifiers.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\This Pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "10760/10760 [==============================] - 3s 264us/step - loss: 0.6790 - acc: 0.5651\n",
      "Epoch 2/100\n",
      "10760/10760 [==============================] - 2s 229us/step - loss: 0.6578 - acc: 0.6152\n",
      "Epoch 3/100\n",
      "10760/10760 [==============================] - 3s 236us/step - loss: 0.6313 - acc: 0.6520\n",
      "Epoch 4/100\n",
      "10760/10760 [==============================] - 2s 217us/step - loss: 0.5970 - acc: 0.6823\n",
      "Epoch 5/100\n",
      "10760/10760 [==============================] - 2s 212us/step - loss: 0.5690 - acc: 0.6960\n",
      "Epoch 6/100\n",
      "10760/10760 [==============================] - 2s 215us/step - loss: 0.5500 - acc: 0.7068\n",
      "Epoch 7/100\n",
      "10760/10760 [==============================] - 2s 206us/step - loss: 0.5329 - acc: 0.7107\n",
      "Epoch 8/100\n",
      "10760/10760 [==============================] - 2s 210us/step - loss: 0.5232 - acc: 0.7225\n",
      "Epoch 9/100\n",
      "10760/10760 [==============================] - 2s 202us/step - loss: 0.5157 - acc: 0.7224\n",
      "Epoch 10/100\n",
      "10760/10760 [==============================] - 2s 209us/step - loss: 0.5100 - acc: 0.7304\n",
      "Epoch 11/100\n",
      "10760/10760 [==============================] - 2s 212us/step - loss: 0.5051 - acc: 0.7340\n",
      "Epoch 12/100\n",
      "10760/10760 [==============================] - 2s 203us/step - loss: 0.5007 - acc: 0.7329\n",
      "Epoch 13/100\n",
      "10760/10760 [==============================] - 2s 206us/step - loss: 0.4933 - acc: 0.7416\n",
      "Epoch 14/100\n",
      "10760/10760 [==============================] - 2s 199us/step - loss: 0.4845 - acc: 0.7498\n",
      "Epoch 15/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4773 - acc: 0.7533\n",
      "Epoch 16/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4715 - acc: 0.7620\n",
      "Epoch 17/100\n",
      "10760/10760 [==============================] - 2s 199us/step - loss: 0.4647 - acc: 0.7712\n",
      "Epoch 18/100\n",
      "10760/10760 [==============================] - 2s 199us/step - loss: 0.4640 - acc: 0.7671\n",
      "Epoch 19/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4587 - acc: 0.7735\n",
      "Epoch 20/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4571 - acc: 0.7715\n",
      "Epoch 21/100\n",
      "10760/10760 [==============================] - 2s 199us/step - loss: 0.4543 - acc: 0.7786\n",
      "Epoch 22/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4519 - acc: 0.7768\n",
      "Epoch 23/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4489 - acc: 0.7810\n",
      "Epoch 24/100\n",
      "10760/10760 [==============================] - 2s 201us/step - loss: 0.4472 - acc: 0.7844\n",
      "Epoch 25/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4425 - acc: 0.7855\n",
      "Epoch 26/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4379 - acc: 0.7904\n",
      "Epoch 27/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4324 - acc: 0.7927\n",
      "Epoch 28/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4276 - acc: 0.7970\n",
      "Epoch 29/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4291 - acc: 0.7996\n",
      "Epoch 30/100\n",
      "10760/10760 [==============================] - 2s 204us/step - loss: 0.4249 - acc: 0.8004\n",
      "Epoch 31/100\n",
      "10760/10760 [==============================] - 3s 233us/step - loss: 0.4223 - acc: 0.8006\n",
      "Epoch 32/100\n",
      "10760/10760 [==============================] - 2s 206us/step - loss: 0.4223 - acc: 0.8033\n",
      "Epoch 33/100\n",
      "10760/10760 [==============================] - 2s 207us/step - loss: 0.4204 - acc: 0.8035\n",
      "Epoch 34/100\n",
      "10760/10760 [==============================] - 2s 205us/step - loss: 0.4197 - acc: 0.8017\n",
      "Epoch 35/100\n",
      "10760/10760 [==============================] - 2s 204us/step - loss: 0.4153 - acc: 0.8067\n",
      "Epoch 36/100\n",
      "10760/10760 [==============================] - 2s 222us/step - loss: 0.4162 - acc: 0.8080\n",
      "Epoch 37/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4161 - acc: 0.8070\n",
      "Epoch 38/100\n",
      "10760/10760 [==============================] - 2s 203us/step - loss: 0.4136 - acc: 0.8100\n",
      "Epoch 39/100\n",
      "10760/10760 [==============================] - 2s 217us/step - loss: 0.4150 - acc: 0.8051\n",
      "Epoch 40/100\n",
      "10760/10760 [==============================] - 2s 212us/step - loss: 0.4126 - acc: 0.8075\n",
      "Epoch 41/100\n",
      "10760/10760 [==============================] - 2s 199us/step - loss: 0.4103 - acc: 0.8088\n",
      "Epoch 42/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4135 - acc: 0.8091\n",
      "Epoch 43/100\n",
      "10760/10760 [==============================] - 2s 201us/step - loss: 0.4105 - acc: 0.8098\n",
      "Epoch 44/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.4098 - acc: 0.8102\n",
      "Epoch 45/100\n",
      "10760/10760 [==============================] - 3s 233us/step - loss: 0.4071 - acc: 0.8104\n",
      "Epoch 46/100\n",
      "10760/10760 [==============================] - 3s 244us/step - loss: 0.4103 - acc: 0.8092\n",
      "Epoch 47/100\n",
      "10760/10760 [==============================] - 3s 242us/step - loss: 0.4072 - acc: 0.8086\n",
      "Epoch 48/100\n",
      "10760/10760 [==============================] - 2s 224us/step - loss: 0.4056 - acc: 0.8116\n",
      "Epoch 49/100\n",
      "10760/10760 [==============================] - 2s 222us/step - loss: 0.4039 - acc: 0.8099\n",
      "Epoch 50/100\n",
      "10760/10760 [==============================] - 2s 227us/step - loss: 0.4037 - acc: 0.8114\n",
      "Epoch 51/100\n",
      "10760/10760 [==============================] - 3s 241us/step - loss: 0.3989 - acc: 0.8174\n",
      "Epoch 52/100\n",
      "10760/10760 [==============================] - 3s 239us/step - loss: 0.3987 - acc: 0.8125\n",
      "Epoch 53/100\n",
      "10760/10760 [==============================] - 3s 239us/step - loss: 0.3972 - acc: 0.8130\n",
      "Epoch 54/100\n",
      "10760/10760 [==============================] - 2s 227us/step - loss: 0.3994 - acc: 0.8116\n",
      "Epoch 55/100\n",
      "10760/10760 [==============================] - 3s 237us/step - loss: 0.3960 - acc: 0.8153\n",
      "Epoch 56/100\n",
      "10760/10760 [==============================] - 2s 204us/step - loss: 0.3979 - acc: 0.8171\n",
      "Epoch 57/100\n",
      "10760/10760 [==============================] - 2s 203us/step - loss: 0.3976 - acc: 0.8146\n",
      "Epoch 58/100\n",
      "10760/10760 [==============================] - 2s 205us/step - loss: 0.3950 - acc: 0.8172\n",
      "Epoch 59/100\n",
      "10760/10760 [==============================] - 2s 228us/step - loss: 0.3939 - acc: 0.8152\n",
      "Epoch 60/100\n",
      "10760/10760 [==============================] - 2s 219us/step - loss: 0.3955 - acc: 0.8141\n",
      "Epoch 61/100\n",
      "10760/10760 [==============================] - 2s 231us/step - loss: 0.3945 - acc: 0.8154\n",
      "Epoch 62/100\n",
      "10760/10760 [==============================] - 3s 295us/step - loss: 0.3937 - acc: 0.8162\n",
      "Epoch 63/100\n",
      "10760/10760 [==============================] - 3s 277us/step - loss: 0.3939 - acc: 0.8154\n",
      "Epoch 64/100\n",
      "10760/10760 [==============================] - 2s 222us/step - loss: 0.3938 - acc: 0.8151\n",
      "Epoch 65/100\n",
      "10760/10760 [==============================] - 2s 223us/step - loss: 0.3930 - acc: 0.8134\n",
      "Epoch 66/100\n",
      "10760/10760 [==============================] - 2s 215us/step - loss: 0.3943 - acc: 0.8127\n",
      "Epoch 67/100\n",
      "10760/10760 [==============================] - 2s 210us/step - loss: 0.3918 - acc: 0.8158\n",
      "Epoch 68/100\n",
      "10760/10760 [==============================] - 2s 225us/step - loss: 0.3923 - acc: 0.8151\n",
      "Epoch 69/100\n",
      "10760/10760 [==============================] - 2s 223us/step - loss: 0.3917 - acc: 0.8154\n",
      "Epoch 70/100\n",
      "10760/10760 [==============================] - 2s 209us/step - loss: 0.3891 - acc: 0.8149\n",
      "Epoch 71/100\n",
      "10760/10760 [==============================] - 2s 205us/step - loss: 0.3910 - acc: 0.8169\n",
      "Epoch 72/100\n",
      "10760/10760 [==============================] - 3s 238us/step - loss: 0.3902 - acc: 0.8165\n",
      "Epoch 73/100\n",
      "10760/10760 [==============================] - 3s 248us/step - loss: 0.3913 - acc: 0.8191\n",
      "Epoch 74/100\n",
      "10760/10760 [==============================] - 2s 212us/step - loss: 0.3909 - acc: 0.8181\n",
      "Epoch 75/100\n",
      "10760/10760 [==============================] - 2s 203us/step - loss: 0.3914 - acc: 0.8133\n",
      "Epoch 76/100\n",
      "10760/10760 [==============================] - 3s 267us/step - loss: 0.3901 - acc: 0.8165\n",
      "Epoch 77/100\n",
      "10760/10760 [==============================] - 3s 241us/step - loss: 0.3880 - acc: 0.8169\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10760/10760 [==============================] - 3s 280us/step - loss: 0.3902 - acc: 0.8143\n",
      "Epoch 79/100\n",
      "10760/10760 [==============================] - 4s 341us/step - loss: 0.3893 - acc: 0.8166\n",
      "Epoch 80/100\n",
      "10760/10760 [==============================] - 3s 236us/step - loss: 0.3888 - acc: 0.8191\n",
      "Epoch 81/100\n",
      "10760/10760 [==============================] - 2s 206us/step - loss: 0.3897 - acc: 0.8186\n",
      "Epoch 82/100\n",
      "10760/10760 [==============================] - 2s 210us/step - loss: 0.3885 - acc: 0.8191\n",
      "Epoch 83/100\n",
      "10760/10760 [==============================] - 2s 204us/step - loss: 0.3864 - acc: 0.8194\n",
      "Epoch 84/100\n",
      "10760/10760 [==============================] - 2s 203us/step - loss: 0.3869 - acc: 0.8204\n",
      "Epoch 85/100\n",
      "10760/10760 [==============================] - 2s 205us/step - loss: 0.3874 - acc: 0.8179\n",
      "Epoch 86/100\n",
      "10760/10760 [==============================] - 2s 201us/step - loss: 0.3875 - acc: 0.8175\n",
      "Epoch 87/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.3866 - acc: 0.8164\n",
      "Epoch 88/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.3874 - acc: 0.8166\n",
      "Epoch 89/100\n",
      "10760/10760 [==============================] - 2s 200us/step - loss: 0.3882 - acc: 0.8144\n",
      "Epoch 90/100\n",
      "10760/10760 [==============================] - 2s 228us/step - loss: 0.3855 - acc: 0.8216\n",
      "Epoch 91/100\n",
      "10760/10760 [==============================] - 2s 204us/step - loss: 0.3863 - acc: 0.8176\n",
      "Epoch 92/100\n",
      "10760/10760 [==============================] - 3s 282us/step - loss: 0.3854 - acc: 0.8191\n",
      "Epoch 93/100\n",
      "10760/10760 [==============================] - 3s 263us/step - loss: 0.3874 - acc: 0.8175\n",
      "Epoch 94/100\n",
      "10760/10760 [==============================] - 2s 210us/step - loss: 0.3879 - acc: 0.8175\n",
      "Epoch 95/100\n",
      "10760/10760 [==============================] - 2s 210us/step - loss: 0.3858 - acc: 0.8203\n",
      "Epoch 96/100\n",
      "10760/10760 [==============================] - 3s 269us/step - loss: 0.3868 - acc: 0.8187\n",
      "Epoch 97/100\n",
      "10760/10760 [==============================] - 3s 234us/step - loss: 0.3863 - acc: 0.8204\n",
      "Epoch 98/100\n",
      "10760/10760 [==============================] - 2s 230us/step - loss: 0.3845 - acc: 0.8198\n",
      "Epoch 99/100\n",
      "10760/10760 [==============================] - 3s 234us/step - loss: 0.3879 - acc: 0.8184\n",
      "Epoch 100/100\n",
      "10760/10760 [==============================] - 3s 252us/step - loss: 0.3850 - acc: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c1fd73748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train wasting\n",
    "# ---------------\n",
    "\n",
    "classifiers.fit(x_train_wast,y_train_wast, batch_size = 3, epochs = 100, class_weight = c_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction wasting\n",
    "# ------------------\n",
    "\n",
    "y_pred_wast = classifiers.predict(x_test_wast)\n",
    "y_pred_wast = (y_pred_wast > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test data:\n",
      "------------------\n",
      "84.197324\n",
      "\n",
      "------------------------------------\n",
      "[[588  25]\n",
      " [164 419]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86       613\n",
      "           1       0.94      0.72      0.82       583\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1196\n",
      "   macro avg       0.86      0.84      0.84      1196\n",
      "weighted avg       0.86      0.84      0.84      1196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wasting result\n",
    "# ----------------\n",
    "\n",
    "print('for test data:\\n------------------')\n",
    "print(round(accuracy_score(y_test_wast,y_pred_wast)*100, 6))\n",
    "print('\\n------------------------------------')\n",
    "print(confusion_matrix(y_test_wast,y_pred_wast))\n",
    "print(classification_report(y_test_wast,y_pred_wast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st hidden layer & input layer\n",
    "\n",
    "classifiers1.add(Dense(8, input_shape=(15,), activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers1.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd hidden layer\n",
    "\n",
    "classifiers1.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers1.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3rd hidden layer\n",
    "\n",
    "# classifiers1.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers1.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding output layer\n",
    "#--------------------------\n",
    "\n",
    "classifiers1.add(Dense(1, activation='sigmoid',kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling ANN\n",
    "#----------------\n",
    "\n",
    "classifiers1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8508/8508 [==============================] - 2s 274us/step - loss: 0.6743 - acc: 0.5771\n",
      "Epoch 2/100\n",
      "8508/8508 [==============================] - 2s 258us/step - loss: 0.6559 - acc: 0.6188\n",
      "Epoch 3/100\n",
      "8508/8508 [==============================] - 2s 236us/step - loss: 0.6499 - acc: 0.6256\n",
      "Epoch 4/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6462 - acc: 0.6353\n",
      "Epoch 5/100\n",
      "8508/8508 [==============================] - 2s 224us/step - loss: 0.6450 - acc: 0.6323\n",
      "Epoch 6/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6439 - acc: 0.6378\n",
      "Epoch 7/100\n",
      "8508/8508 [==============================] - 2s 221us/step - loss: 0.6430 - acc: 0.6341\n",
      "Epoch 8/100\n",
      "8508/8508 [==============================] - 2s 257us/step - loss: 0.6424 - acc: 0.6381\n",
      "Epoch 9/100\n",
      "8508/8508 [==============================] - 2s 234us/step - loss: 0.6429 - acc: 0.6378\n",
      "Epoch 10/100\n",
      "8508/8508 [==============================] - 2s 227us/step - loss: 0.6414 - acc: 0.6394\n",
      "Epoch 11/100\n",
      "8508/8508 [==============================] - 2s 221us/step - loss: 0.6423 - acc: 0.6395\n",
      "Epoch 12/100\n",
      "8508/8508 [==============================] - 2s 217us/step - loss: 0.6413 - acc: 0.6442\n",
      "Epoch 13/100\n",
      "8508/8508 [==============================] - 2s 226us/step - loss: 0.6422 - acc: 0.6389\n",
      "Epoch 14/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6407 - acc: 0.6399\n",
      "Epoch 15/100\n",
      "8508/8508 [==============================] - 2s 222us/step - loss: 0.6398 - acc: 0.6399\n",
      "Epoch 16/100\n",
      "8508/8508 [==============================] - 2s 226us/step - loss: 0.6408 - acc: 0.6394\n",
      "Epoch 17/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6391 - acc: 0.6423\n",
      "Epoch 18/100\n",
      "8508/8508 [==============================] - 2s 224us/step - loss: 0.6390 - acc: 0.6427\n",
      "Epoch 19/100\n",
      "8508/8508 [==============================] - 2s 249us/step - loss: 0.6378 - acc: 0.6439\n",
      "Epoch 20/100\n",
      "8508/8508 [==============================] - 2s 233us/step - loss: 0.6378 - acc: 0.6432\n",
      "Epoch 21/100\n",
      "8508/8508 [==============================] - 2s 222us/step - loss: 0.6383 - acc: 0.6394\n",
      "Epoch 22/100\n",
      "8508/8508 [==============================] - 2s 238us/step - loss: 0.6379 - acc: 0.6446\n",
      "Epoch 23/100\n",
      "8508/8508 [==============================] - 2s 241us/step - loss: 0.6380 - acc: 0.6408\n",
      "Epoch 24/100\n",
      "8508/8508 [==============================] - 2s 246us/step - loss: 0.6380 - acc: 0.6427\n",
      "Epoch 25/100\n",
      "8508/8508 [==============================] - 2s 251us/step - loss: 0.6375 - acc: 0.6400\n",
      "Epoch 26/100\n",
      "8508/8508 [==============================] - 2s 228us/step - loss: 0.6369 - acc: 0.6399\n",
      "Epoch 27/100\n",
      "8508/8508 [==============================] - 2s 226us/step - loss: 0.6377 - acc: 0.6442\n",
      "Epoch 28/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6379 - acc: 0.6412\n",
      "Epoch 29/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6367 - acc: 0.6462\n",
      "Epoch 30/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6362 - acc: 0.6456\n",
      "Epoch 31/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6357 - acc: 0.6450\n",
      "Epoch 32/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6356 - acc: 0.6440\n",
      "Epoch 33/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6360 - acc: 0.6432\n",
      "Epoch 34/100\n",
      "8508/8508 [==============================] - 2s 272us/step - loss: 0.6350 - acc: 0.6473\n",
      "Epoch 35/100\n",
      "8508/8508 [==============================] - 2s 229us/step - loss: 0.6356 - acc: 0.6450\n",
      "Epoch 36/100\n",
      "8508/8508 [==============================] - 2s 230us/step - loss: 0.6349 - acc: 0.6428\n",
      "Epoch 37/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6341 - acc: 0.6460\n",
      "Epoch 38/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6349 - acc: 0.6448\n",
      "Epoch 39/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6338 - acc: 0.6480\n",
      "Epoch 40/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6338 - acc: 0.6484\n",
      "Epoch 41/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6336 - acc: 0.6468\n",
      "Epoch 42/100\n",
      "8508/8508 [==============================] - 2s 270us/step - loss: 0.6343 - acc: 0.6490\n",
      "Epoch 43/100\n",
      "8508/8508 [==============================] - 2s 246us/step - loss: 0.6331 - acc: 0.6521\n",
      "Epoch 44/100\n",
      "8508/8508 [==============================] - 2s 227us/step - loss: 0.6330 - acc: 0.6508\n",
      "Epoch 45/100\n",
      "8508/8508 [==============================] - 2s 228us/step - loss: 0.6325 - acc: 0.6479\n",
      "Epoch 46/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6322 - acc: 0.6479\n",
      "Epoch 47/100\n",
      "8508/8508 [==============================] - 2s 226us/step - loss: 0.6322 - acc: 0.6500\n",
      "Epoch 48/100\n",
      "8508/8508 [==============================] - 2s 290us/step - loss: 0.6328 - acc: 0.6465\n",
      "Epoch 49/100\n",
      "8508/8508 [==============================] - 3s 299us/step - loss: 0.6319 - acc: 0.6509\n",
      "Epoch 50/100\n",
      "8508/8508 [==============================] - 2s 239us/step - loss: 0.6323 - acc: 0.6487\n",
      "Epoch 51/100\n",
      "8508/8508 [==============================] - 2s 230us/step - loss: 0.6319 - acc: 0.6554\n",
      "Epoch 52/100\n",
      "8508/8508 [==============================] - 2s 263us/step - loss: 0.6313 - acc: 0.6480\n",
      "Epoch 53/100\n",
      "8508/8508 [==============================] - 2s 275us/step - loss: 0.6320 - acc: 0.6497\n",
      "Epoch 54/100\n",
      "8508/8508 [==============================] - 2s 257us/step - loss: 0.6314 - acc: 0.6507\n",
      "Epoch 55/100\n",
      "8508/8508 [==============================] - 2s 228us/step - loss: 0.6316 - acc: 0.6530\n",
      "Epoch 56/100\n",
      "8508/8508 [==============================] - 2s 258us/step - loss: 0.6316 - acc: 0.6490\n",
      "Epoch 57/100\n",
      "8508/8508 [==============================] - 2s 237us/step - loss: 0.6308 - acc: 0.6487\n",
      "Epoch 58/100\n",
      "8508/8508 [==============================] - ETA: 0s - loss: 0.6303 - acc: 0.652 - 2s 231us/step - loss: 0.6313 - acc: 0.6512\n",
      "Epoch 59/100\n",
      "8508/8508 [==============================] - 2s 281us/step - loss: 0.6306 - acc: 0.6534\n",
      "Epoch 60/100\n",
      "8508/8508 [==============================] - 2s 233us/step - loss: 0.6312 - acc: 0.6497\n",
      "Epoch 61/100\n",
      "8508/8508 [==============================] - 2s 266us/step - loss: 0.6305 - acc: 0.6508\n",
      "Epoch 62/100\n",
      "8508/8508 [==============================] - 2s 250us/step - loss: 0.6302 - acc: 0.6509\n",
      "Epoch 63/100\n",
      "8508/8508 [==============================] - 2s 229us/step - loss: 0.6302 - acc: 0.6512\n",
      "Epoch 64/100\n",
      "8508/8508 [==============================] - 2s 244us/step - loss: 0.6302 - acc: 0.6515\n",
      "Epoch 65/100\n",
      "8508/8508 [==============================] - 2s 229us/step - loss: 0.6300 - acc: 0.6515\n",
      "Epoch 66/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6293 - acc: 0.6483\n",
      "Epoch 67/100\n",
      "8508/8508 [==============================] - 2s 250us/step - loss: 0.6303 - acc: 0.6543\n",
      "Epoch 68/100\n",
      "8508/8508 [==============================] - 2s 264us/step - loss: 0.6299 - acc: 0.6517\n",
      "Epoch 69/100\n",
      "8508/8508 [==============================] - 2s 273us/step - loss: 0.6299 - acc: 0.6488\n",
      "Epoch 70/100\n",
      "8508/8508 [==============================] - 2s 246us/step - loss: 0.6305 - acc: 0.6529\n",
      "Epoch 71/100\n",
      "8508/8508 [==============================] - 2s 228us/step - loss: 0.6288 - acc: 0.6540\n",
      "Epoch 72/100\n",
      "8508/8508 [==============================] - 2s 228us/step - loss: 0.6295 - acc: 0.6524\n",
      "Epoch 73/100\n",
      "8508/8508 [==============================] - 2s 219us/step - loss: 0.6286 - acc: 0.6487\n",
      "Epoch 74/100\n",
      "8508/8508 [==============================] - 2s 221us/step - loss: 0.6293 - acc: 0.6506\n",
      "Epoch 75/100\n",
      "8508/8508 [==============================] - 2s 225us/step - loss: 0.6280 - acc: 0.6537\n",
      "Epoch 76/100\n",
      "8508/8508 [==============================] - 2s 226us/step - loss: 0.6286 - acc: 0.6534\n",
      "Epoch 77/100\n",
      "8508/8508 [==============================] - 2s 218us/step - loss: 0.6287 - acc: 0.6550\n",
      "Epoch 78/100\n",
      "8508/8508 [==============================] - 2s 258us/step - loss: 0.6284 - acc: 0.6507\n",
      "Epoch 79/100\n",
      "8508/8508 [==============================] - 2s 252us/step - loss: 0.6279 - acc: 0.6510\n",
      "Epoch 80/100\n",
      "8508/8508 [==============================] - 2s 286us/step - loss: 0.6287 - acc: 0.6495\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8508/8508 [==============================] - 2s 233us/step - loss: 0.6292 - acc: 0.6500\n",
      "Epoch 82/100\n",
      "8508/8508 [==============================] - 2s 257us/step - loss: 0.6285 - acc: 0.6547 1s - loss\n",
      "Epoch 83/100\n",
      "8508/8508 [==============================] - 2s 246us/step - loss: 0.6285 - acc: 0.6547\n",
      "Epoch 84/100\n",
      "8508/8508 [==============================] - 2s 271us/step - loss: 0.6278 - acc: 0.6529\n",
      "Epoch 85/100\n",
      "8508/8508 [==============================] - 2s 257us/step - loss: 0.6283 - acc: 0.6507\n",
      "Epoch 86/100\n",
      "8508/8508 [==============================] - 3s 300us/step - loss: 0.6282 - acc: 0.6536\n",
      "Epoch 87/100\n",
      "8508/8508 [==============================] - 2s 286us/step - loss: 0.6278 - acc: 0.6523\n",
      "Epoch 88/100\n",
      "8508/8508 [==============================] - 2s 290us/step - loss: 0.6273 - acc: 0.6523\n",
      "Epoch 89/100\n",
      "8508/8508 [==============================] - 2s 253us/step - loss: 0.6274 - acc: 0.6517\n",
      "Epoch 90/100\n",
      "8508/8508 [==============================] - 2s 280us/step - loss: 0.6280 - acc: 0.6521\n",
      "Epoch 91/100\n",
      "8508/8508 [==============================] - 3s 311us/step - loss: 0.6274 - acc: 0.6535\n",
      "Epoch 92/100\n",
      "8508/8508 [==============================] - 3s 389us/step - loss: 0.6278 - acc: 0.6549\n",
      "Epoch 93/100\n",
      "8508/8508 [==============================] - 2s 254us/step - loss: 0.6274 - acc: 0.6529\n",
      "Epoch 94/100\n",
      "8508/8508 [==============================] - 2s 217us/step - loss: 0.6275 - acc: 0.6561\n",
      "Epoch 95/100\n",
      "8508/8508 [==============================] - 2s 216us/step - loss: 0.6273 - acc: 0.6555\n",
      "Epoch 96/100\n",
      "8508/8508 [==============================] - 2s 217us/step - loss: 0.6269 - acc: 0.6555\n",
      "Epoch 97/100\n",
      "8508/8508 [==============================] - 2s 247us/step - loss: 0.6279 - acc: 0.6504\n",
      "Epoch 98/100\n",
      "8508/8508 [==============================] - 3s 307us/step - loss: 0.6278 - acc: 0.6508\n",
      "Epoch 99/100\n",
      "8508/8508 [==============================] - 2s 256us/step - loss: 0.6277 - acc: 0.6507\n",
      "Epoch 100/100\n",
      "8508/8508 [==============================] - 2s 233us/step - loss: 0.6268 - acc: 0.6575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c203085f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train underweight\n",
    "# ---------------\n",
    "\n",
    "classifiers1.fit(x_train_under,y_train_under, batch_size = 3, epochs = 100, class_weight = c_weight1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction underweight\n",
    "# ------------------\n",
    "\n",
    "y_pred_under = classifiers1.predict(x_test_under)\n",
    "y_pred_under = (y_pred_under > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test data:\n",
      "------------------\n",
      "62.896406\n",
      "\n",
      "------------------------------------\n",
      "[[240 217]\n",
      " [134 355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58       457\n",
      "           1       0.62      0.73      0.67       489\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       946\n",
      "   macro avg       0.63      0.63      0.62       946\n",
      "weighted avg       0.63      0.63      0.62       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# underweight result\n",
    "# ----------------\n",
    "\n",
    "print('for test data:\\n------------------')\n",
    "print(round(accuracy_score(y_test_under,y_pred_under)*100, 6))\n",
    "print('\\n------------------------------------')\n",
    "print(confusion_matrix(y_test_under,y_pred_under))\n",
    "print(classification_report(y_test_under,y_pred_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st hidden layer & input layer\n",
    "\n",
    "classifiers2.add(Dense(8, input_shape=(15,), activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers2.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd hidden layer\n",
    "\n",
    "classifiers2.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers2.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3rd hidden layer\n",
    "\n",
    "# classifiers2.add(Dense(8, activation='relu',kernel_initializer='uniform')) \n",
    "# classifiers2.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding output layer\n",
    "#--------------------------\n",
    "\n",
    "classifiers2.add(Dense(1, activation='sigmoid',kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling ANN\n",
    "#----------------\n",
    "\n",
    "classifiers2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8010/8010 [==============================] - 2s 283us/step - loss: 0.6659 - acc: 0.6055\n",
      "Epoch 2/100\n",
      "8010/8010 [==============================] - 2s 253us/step - loss: 0.6519 - acc: 0.6236\n",
      "Epoch 3/100\n",
      "8010/8010 [==============================] - 2s 302us/step - loss: 0.6452 - acc: 0.6373\n",
      "Epoch 4/100\n",
      "8010/8010 [==============================] - 2s 292us/step - loss: 0.6380 - acc: 0.6486\n",
      "Epoch 5/100\n",
      "8010/8010 [==============================] - 2s 292us/step - loss: 0.6333 - acc: 0.6536\n",
      "Epoch 6/100\n",
      "8010/8010 [==============================] - 2s 301us/step - loss: 0.6301 - acc: 0.6568\n",
      "Epoch 7/100\n",
      "8010/8010 [==============================] - 2s 310us/step - loss: 0.6277 - acc: 0.6554\n",
      "Epoch 8/100\n",
      "8010/8010 [==============================] - 2s 279us/step - loss: 0.6258 - acc: 0.6610\n",
      "Epoch 9/100\n",
      "8010/8010 [==============================] - 2s 276us/step - loss: 0.6247 - acc: 0.6652\n",
      "Epoch 10/100\n",
      "8010/8010 [==============================] - 2s 290us/step - loss: 0.6230 - acc: 0.6697\n",
      "Epoch 11/100\n",
      "8010/8010 [==============================] - 2s 306us/step - loss: 0.6214 - acc: 0.6659\n",
      "Epoch 12/100\n",
      "8010/8010 [==============================] - 2s 311us/step - loss: 0.6205 - acc: 0.6688\n",
      "Epoch 13/100\n",
      "8010/8010 [==============================] - 2s 231us/step - loss: 0.6194 - acc: 0.6692\n",
      "Epoch 14/100\n",
      "8010/8010 [==============================] - 2s 248us/step - loss: 0.6186 - acc: 0.6692\n",
      "Epoch 15/100\n",
      "8010/8010 [==============================] - 2s 305us/step - loss: 0.6185 - acc: 0.6728\n",
      "Epoch 16/100\n",
      "8010/8010 [==============================] - 2s 294us/step - loss: 0.6171 - acc: 0.6735\n",
      "Epoch 17/100\n",
      "8010/8010 [==============================] - 2s 270us/step - loss: 0.6170 - acc: 0.6732\n",
      "Epoch 18/100\n",
      "8010/8010 [==============================] - 2s 286us/step - loss: 0.6163 - acc: 0.6703\n",
      "Epoch 19/100\n",
      "8010/8010 [==============================] - 2s 254us/step - loss: 0.6158 - acc: 0.6743\n",
      "Epoch 20/100\n",
      "8010/8010 [==============================] - 2s 242us/step - loss: 0.6151 - acc: 0.6730\n",
      "Epoch 21/100\n",
      "8010/8010 [==============================] - 2s 276us/step - loss: 0.6151 - acc: 0.6729\n",
      "Epoch 22/100\n",
      "8010/8010 [==============================] - 2s 293us/step - loss: 0.6133 - acc: 0.6748\n",
      "Epoch 23/100\n",
      "8010/8010 [==============================] - 2s 282us/step - loss: 0.6129 - acc: 0.6793\n",
      "Epoch 24/100\n",
      "8010/8010 [==============================] - 2s 285us/step - loss: 0.6127 - acc: 0.6785\n",
      "Epoch 25/100\n",
      "8010/8010 [==============================] - 2s 283us/step - loss: 0.6129 - acc: 0.6772\n",
      "Epoch 26/100\n",
      "8010/8010 [==============================] - 2s 252us/step - loss: 0.6129 - acc: 0.6755\n",
      "Epoch 27/100\n",
      "8010/8010 [==============================] - 2s 262us/step - loss: 0.6112 - acc: 0.6755\n",
      "Epoch 28/100\n",
      "8010/8010 [==============================] - 2s 218us/step - loss: 0.6114 - acc: 0.6754\n",
      "Epoch 29/100\n",
      "8010/8010 [==============================] - 2s 244us/step - loss: 0.6119 - acc: 0.6780\n",
      "Epoch 30/100\n",
      "8010/8010 [==============================] - 2s 244us/step - loss: 0.6108 - acc: 0.6801\n",
      "Epoch 31/100\n",
      "8010/8010 [==============================] - 2s 228us/step - loss: 0.6109 - acc: 0.6755\n",
      "Epoch 32/100\n",
      "8010/8010 [==============================] - 2s 270us/step - loss: 0.6094 - acc: 0.6792\n",
      "Epoch 33/100\n",
      "8010/8010 [==============================] - 2s 250us/step - loss: 0.6098 - acc: 0.6811\n",
      "Epoch 34/100\n",
      "8010/8010 [==============================] - 2s 252us/step - loss: 0.6088 - acc: 0.6814\n",
      "Epoch 35/100\n",
      "8010/8010 [==============================] - 2s 275us/step - loss: 0.6086 - acc: 0.6792\n",
      "Epoch 36/100\n",
      "8010/8010 [==============================] - 2s 225us/step - loss: 0.6081 - acc: 0.6804\n",
      "Epoch 37/100\n",
      "8010/8010 [==============================] - 2s 245us/step - loss: 0.6079 - acc: 0.6816\n",
      "Epoch 38/100\n",
      "8010/8010 [==============================] - 2s 232us/step - loss: 0.6069 - acc: 0.6823\n",
      "Epoch 39/100\n",
      "8010/8010 [==============================] - 2s 248us/step - loss: 0.6070 - acc: 0.6821\n",
      "Epoch 40/100\n",
      "8010/8010 [==============================] - 2s 218us/step - loss: 0.6058 - acc: 0.6810\n",
      "Epoch 41/100\n",
      "8010/8010 [==============================] - 2s 231us/step - loss: 0.6063 - acc: 0.6826\n",
      "Epoch 42/100\n",
      "8010/8010 [==============================] - 2s 227us/step - loss: 0.6056 - acc: 0.6878\n",
      "Epoch 43/100\n",
      "8010/8010 [==============================] - 2s 246us/step - loss: 0.6050 - acc: 0.6855\n",
      "Epoch 44/100\n",
      "8010/8010 [==============================] - 2s 225us/step - loss: 0.6045 - acc: 0.6815\n",
      "Epoch 45/100\n",
      "8010/8010 [==============================] - 2s 229us/step - loss: 0.6038 - acc: 0.6816\n",
      "Epoch 46/100\n",
      "8010/8010 [==============================] - 2s 241us/step - loss: 0.6026 - acc: 0.6861\n",
      "Epoch 47/100\n",
      "8010/8010 [==============================] - 2s 266us/step - loss: 0.6036 - acc: 0.6850\n",
      "Epoch 48/100\n",
      "8010/8010 [==============================] - 2s 231us/step - loss: 0.6024 - acc: 0.6869\n",
      "Epoch 49/100\n",
      "8010/8010 [==============================] - 2s 224us/step - loss: 0.6013 - acc: 0.6821\n",
      "Epoch 50/100\n",
      "8010/8010 [==============================] - 2s 219us/step - loss: 0.6000 - acc: 0.6858\n",
      "Epoch 51/100\n",
      "8010/8010 [==============================] - 2s 226us/step - loss: 0.5978 - acc: 0.6853\n",
      "Epoch 52/100\n",
      "8010/8010 [==============================] - 2s 220us/step - loss: 0.5969 - acc: 0.6856\n",
      "Epoch 53/100\n",
      "8010/8010 [==============================] - 2s 210us/step - loss: 0.5947 - acc: 0.6845\n",
      "Epoch 54/100\n",
      "8010/8010 [==============================] - 2s 223us/step - loss: 0.5925 - acc: 0.6838\n",
      "Epoch 55/100\n",
      "8010/8010 [==============================] - 2s 218us/step - loss: 0.5908 - acc: 0.6856\n",
      "Epoch 56/100\n",
      "8010/8010 [==============================] - 2s 227us/step - loss: 0.5880 - acc: 0.6916\n",
      "Epoch 57/100\n",
      "8010/8010 [==============================] - 2s 253us/step - loss: 0.5885 - acc: 0.6888\n",
      "Epoch 58/100\n",
      "8010/8010 [==============================] - 2s 221us/step - loss: 0.5862 - acc: 0.6893\n",
      "Epoch 59/100\n",
      "8010/8010 [==============================] - 2s 232us/step - loss: 0.5856 - acc: 0.6888\n",
      "Epoch 60/100\n",
      "8010/8010 [==============================] - 2s 248us/step - loss: 0.5842 - acc: 0.6900\n",
      "Epoch 61/100\n",
      "8010/8010 [==============================] - 2s 221us/step - loss: 0.5830 - acc: 0.6924\n",
      "Epoch 62/100\n",
      "8010/8010 [==============================] - 2s 224us/step - loss: 0.5811 - acc: 0.6899\n",
      "Epoch 63/100\n",
      "8010/8010 [==============================] - 2s 224us/step - loss: 0.5803 - acc: 0.6921\n",
      "Epoch 64/100\n",
      "8010/8010 [==============================] - 2s 238us/step - loss: 0.5795 - acc: 0.6918\n",
      "Epoch 65/100\n",
      "8010/8010 [==============================] - 2s 270us/step - loss: 0.5780 - acc: 0.6936\n",
      "Epoch 66/100\n",
      "8010/8010 [==============================] - 2s 256us/step - loss: 0.5757 - acc: 0.6943\n",
      "Epoch 67/100\n",
      "8010/8010 [==============================] - 2s 244us/step - loss: 0.5747 - acc: 0.6953\n",
      "Epoch 68/100\n",
      "8010/8010 [==============================] - 2s 253us/step - loss: 0.5740 - acc: 0.6985\n",
      "Epoch 69/100\n",
      "8010/8010 [==============================] - 2s 244us/step - loss: 0.5732 - acc: 0.6959\n",
      "Epoch 70/100\n",
      "8010/8010 [==============================] - 2s 250us/step - loss: 0.5735 - acc: 0.6995\n",
      "Epoch 71/100\n",
      "8010/8010 [==============================] - 2s 243us/step - loss: 0.5715 - acc: 0.6960\n",
      "Epoch 72/100\n",
      "8010/8010 [==============================] - 2s 254us/step - loss: 0.5727 - acc: 0.6946\n",
      "Epoch 73/100\n",
      "8010/8010 [==============================] - 2s 238us/step - loss: 0.5723 - acc: 0.6958\n",
      "Epoch 74/100\n",
      "8010/8010 [==============================] - 2s 262us/step - loss: 0.5700 - acc: 0.6953\n",
      "Epoch 75/100\n",
      "8010/8010 [==============================] - 2s 248us/step - loss: 0.5708 - acc: 0.6953\n",
      "Epoch 76/100\n",
      "8010/8010 [==============================] - 2s 252us/step - loss: 0.5695 - acc: 0.6933\n",
      "Epoch 77/100\n",
      "8010/8010 [==============================] - 2s 254us/step - loss: 0.5688 - acc: 0.6966\n",
      "Epoch 78/100\n",
      "8010/8010 [==============================] - 2s 239us/step - loss: 0.5693 - acc: 0.6954\n",
      "Epoch 79/100\n",
      "8010/8010 [==============================] - 2s 236us/step - loss: 0.5693 - acc: 0.7004\n",
      "Epoch 80/100\n",
      "8010/8010 [==============================] - 2s 226us/step - loss: 0.5683 - acc: 0.7004\n",
      "Epoch 81/100\n",
      "8010/8010 [==============================] - 2s 225us/step - loss: 0.5678 - acc: 0.6984\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8010/8010 [==============================] - 2s 232us/step - loss: 0.5667 - acc: 0.6991\n",
      "Epoch 83/100\n",
      "8010/8010 [==============================] - 2s 223us/step - loss: 0.5665 - acc: 0.6974\n",
      "Epoch 84/100\n",
      "8010/8010 [==============================] - 2s 236us/step - loss: 0.5672 - acc: 0.6981\n",
      "Epoch 85/100\n",
      "8010/8010 [==============================] - 2s 230us/step - loss: 0.5663 - acc: 0.6986\n",
      "Epoch 86/100\n",
      "8010/8010 [==============================] - 2s 227us/step - loss: 0.5660 - acc: 0.6949\n",
      "Epoch 87/100\n",
      "8010/8010 [==============================] - 2s 240us/step - loss: 0.5650 - acc: 0.6946\n",
      "Epoch 88/100\n",
      "8010/8010 [==============================] - 2s 234us/step - loss: 0.5657 - acc: 0.6968\n",
      "Epoch 89/100\n",
      "8010/8010 [==============================] - 2s 224us/step - loss: 0.5657 - acc: 0.6991\n",
      "Epoch 90/100\n",
      "8010/8010 [==============================] - 2s 226us/step - loss: 0.5638 - acc: 0.6998\n",
      "Epoch 91/100\n",
      "8010/8010 [==============================] - 2s 225us/step - loss: 0.5645 - acc: 0.6969\n",
      "Epoch 92/100\n",
      "8010/8010 [==============================] - 2s 226us/step - loss: 0.5645 - acc: 0.6960\n",
      "Epoch 93/100\n",
      "8010/8010 [==============================] - 2s 224us/step - loss: 0.5624 - acc: 0.6985\n",
      "Epoch 94/100\n",
      "8010/8010 [==============================] - 2s 248us/step - loss: 0.5632 - acc: 0.6963\n",
      "Epoch 95/100\n",
      "8010/8010 [==============================] - 2s 261us/step - loss: 0.5634 - acc: 0.6975\n",
      "Epoch 96/100\n",
      "8010/8010 [==============================] - 2s 238us/step - loss: 0.5634 - acc: 0.6981\n",
      "Epoch 97/100\n",
      "8010/8010 [==============================] - 2s 261us/step - loss: 0.5624 - acc: 0.7024\n",
      "Epoch 98/100\n",
      "8010/8010 [==============================] - 2s 238us/step - loss: 0.5610 - acc: 0.6986\n",
      "Epoch 99/100\n",
      "8010/8010 [==============================] - 2s 239us/step - loss: 0.5617 - acc: 0.6970\n",
      "Epoch 100/100\n",
      "8010/8010 [==============================] - 2s 264us/step - loss: 0.5623 - acc: 0.6979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c224cca20>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train stunting\n",
    "# ---------------\n",
    "\n",
    "classifiers2.fit(x_train_stun,y_train_stun, batch_size = 3, epochs = 100, class_weight = c_weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction stunting\n",
    "# ------------------\n",
    "\n",
    "y_pred_stun = classifiers2.predict(x_test_stun)\n",
    "y_pred_stun = (y_pred_stun > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test data:\n",
      "------------------\n",
      "71.235955\n",
      "\n",
      "------------------------------------\n",
      "[[367  83]\n",
      " [173 267]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.82      0.74       450\n",
      "           1       0.76      0.61      0.68       440\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       890\n",
      "   macro avg       0.72      0.71      0.71       890\n",
      "weighted avg       0.72      0.71      0.71       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stunting result\n",
    "# ----------------\n",
    "\n",
    "print('for test data:\\n------------------')\n",
    "print(round(accuracy_score(y_test_stun,y_pred_stun)*100, 6))\n",
    "print('\\n------------------------------------')\n",
    "print(confusion_matrix(y_test_stun,y_pred_stun))\n",
    "print(classification_report(y_test_stun,y_pred_stun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting stunting, underweight & wasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother age at first birth (in year): 16\n",
      "child age (in month): 13\n",
      "diarrhea [yes(1.0), no(0.0)]: 0.0\n",
      "fever [yes(1.0), no(0.0)]: 1.0\n",
      "ari [yes(1), no(0)]: 1\n",
      "mother BMI: 20.96\n",
      "birth order: 2\n",
      "mother education [yes(1), no(0)]: 0\n",
      "wealth condition [rich(1), poor(0)]: 0\n",
      "Father education [yes(1.0), no(0)]: 0.0\n",
      "residence [rural(1) , urban(0)]: 1\n",
      "gender[male(0), female(1)]: 1\n",
      "mother working [yes(1.0), no(0.0)]: 0.0\n",
      "breastfeeding [yes(1), no(0)]: 1\n",
      "no of family members: 6\n"
     ]
    }
   ],
   "source": [
    "age_at_first_bir = input('mother age at first birth (in year): ' )\n",
    "child_age        = input('child age (in month): ' )\n",
    "diar             = float(input('diarrhea [yes(1.0), no(0.0)]: '))\n",
    "fever            = float(input('fever [yes(1.0), no(0.0)]: '))\n",
    "ari              = int(input('ari [yes(1), no(0)]: '))\n",
    "mother_BMI       = float(input('mother BMI: '))\n",
    "birth_order      = int(input('birth order: '))\n",
    "mother_edu       = int(input('mother education [yes(1), no(0)]: '))\n",
    "wealth_condition = int(input('wealth condition [rich(1), poor(0)]: '))\n",
    "Father_Edu       = float(input('Father education [yes(1.0), no(0)]: '))\n",
    "residence        = int(input('residence [rural(1) , urban(0)]: '))\n",
    "gender           = int(input('gender[male(0), female(1)]: ')) \n",
    "mother_working   = float(input('mother working [yes(1.0), no(0.0)]: '))\n",
    "breastfeeding    = int(input('breastfeeding [yes(1), no(0)]: '))\n",
    "household_no     = int(input('no of family members: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stun  = ''\n",
    "under = ''\n",
    "was   = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_stun = classifiers2.predict(sc.transform(np.array([[age_at_first_bir, child_age, diar, fever, ari, mother_BMI, birth_order, mother_edu, wealth_condition, Father_Edu, residence, gender, mother_working, breastfeeding, household_no]])))\n",
    "new_stun = classifiers2.predict(sc.transform(np.array([[18, 47, 0.0, 1.0, 0, 19.71, 2, 0, 0, 0.0, 1, 1, 0.0, 0, 5]])))\n",
    "new_stun = (new_stun > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_under = classifiers1.predict(sc.transform(np.array([[age_at_first_bir, child_age, diar, fever, ari, mother_BMI, birth_order, mother_edu, wealth_condition, Father_Edu, residence, gender, mother_working, breastfeeding, household_no]])))\n",
    "new_under = classifiers1.predict(sc.transform(np.array([[18, 47, 0.0, 1.0, 0, 19.71, 2, 0, 0, 0.0, 1, 1, 0.0, 0, 5]])))\n",
    "new_under = (new_under > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_was = classifiers.predict(sc.transform(np.array([[age_at_first_bir, child_age, diar, fever, ari, mother_BMI, birth_order, mother_edu, wealth_condition, Father_Edu, residence, gender, mother_working, breastfeeding, household_no]])))\n",
    "new_was = classifiers.predict(sc.transform(np.array([[18, 47, 0.0, 1.0, 0, 19.71, 2, 0, 0, 0.0, 1, 1, 0.0, 0, 5]])))\n",
    "new_was = (new_was > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_stun:\n",
    "    for j in i:\n",
    "        if j == True:\n",
    "            stun = 'not stunting'\n",
    "        else:\n",
    "            stun = 'stunting'\n",
    "\n",
    "for i in new_under:\n",
    "    for j in i:\n",
    "        if j == True:\n",
    "            under = 'not underweight'\n",
    "        else:\n",
    "            under = 'underweight'\n",
    "\n",
    "for i in new_was:\n",
    "    for j in i:\n",
    "        if j == True:\n",
    "            was = 'not wasting'\n",
    "        else:\n",
    "            was = 'wasting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child is:\n",
      " -------------------\n",
      " #  not stunting ;\n",
      "\n",
      " #  underweight ;\n",
      "\n",
      " #  wasting ;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('child is:\\n -------------------\\n # ', stun, ';\\n\\n' , '# ',under , ';\\n\\n' , '# ',was,';')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
